\hypertarget{redes-neuronales}{%
\section{Redes neuronales}\label{redes-neuronales}}

\hypertarget{neurona}{%
\subsection{Neurona}\label{neurona}}

\hypertarget{activaciuxf3n}{%
\subsubsection{Activación}\label{activaciuxf3n}}

\hypertarget{observaciuxf3n-funciuxf3n-de-activaciuxf3n-linealpolinuxf3mica.}{%
\paragraph{Observación: función de activación
lineal/polinómica.}\label{observaciuxf3n-funciuxf3n-de-activaciuxf3n-linealpolinuxf3mica.}}

dónde poner lo de las activaciones no polinomiales - que bastan para
aproximar cualquier función continua. Imagino en el apartado de
convergence theorems.

Resultado: una red neuronal con función de activación lineal/polinómica
es equivalente a \ldots{} (algo mucho más simple TODO). En el caso
lineal o afín la RN se convierte en una recta ella misma. En el caso
polinómico, en un polinomio, que con profundidad infinita puede
aproximar cualquier función continua\ldots{} A leer el paper de
non-polynomial activation functions \textless=\textgreater{} learns
anything

Numeric analysis observation: gotta be careful with vanishing/exploding
gradients (unbounded derivatives)

De la manera más general posible, la familia de redes neuronales es un
conjunto de funciones \(f: X \to Y\), donde \(X\) e \(Y\) son espacios
vectoriales sobre un cuerpo \(\mathbb{K}\). A su misma vez, \(f\) es una
composición de funciones \(f = \sigma \circ A\), donde \(A: X \to Y\) es
una aplicación afín y \(\sigma: Y \to \mathbb{K}\) es una función de
activación.

\hypertarget{feedforward}{%
\subsection{Feedforward}\label{feedforward}}

\hypertarget{backpropagation}{%
\subsection{Backpropagation}\label{backpropagation}}

\hypertarget{gradient-descent}{%
\subsection{Gradient descent (?)}\label{gradient-descent}}

\hypertarget{stochastic-gradient-descent}{%
\subsubsection{Stochastic Gradient
Descent}\label{stochastic-gradient-descent}}

\hypertarget{convergence-theorems-for-neural-networks}{%
\subsection{Convecopia/plain-es.bst copia/alpha-es.bstrgence theorems for neural
networks}\label{convergence-theorems-for-neural-networks}}

\hypertarget{universal-approximation-theorem}{%
\subsubsection{Universal approximation
theorem}\label{universal-approximation-theorem}}

\hypertarget{arbitrary-width}{%
\subsubsection{Arbitrary width}\label{arbitrary-width}}

\hypertarget{arbitrary-depth}{%
\subsubsection{Arbitrary depth}\label{arbitrary-depth}}
