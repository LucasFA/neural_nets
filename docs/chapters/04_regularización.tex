\hypertarget{regularizaciuxf3n}{%
\section{Regularización}\label{regularizaciuxf3n}}

\hypertarget{motivaciuxf3n}{%
\subsection{Motivación}\label{motivaciuxf3n}}

Overfitting is blah blah, ecuación de inferencia MSE = bias \^{} 2 +
varianza paper de Yann Le Cun de Learning in high dimensions always
amounts to extrapolation (according to the paper even if it's a
low-dimensional data manifold! TBResearched)

\hypertarget{implicit-biases}{%
\subsection{Implicit biases}\label{implicit-biases}}

Wikipedia da enlaces a ciertos papers interesantes sobre el ``sesgo
espectral'' frequency domain bias - aparentemente las redes neuronales
tienden a aproximar primero frecuencias bajas - en el sentido de
Fourier: On the Spectral Bias of Neural Networks
https://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf

Training behavior of deep neural network in frequency domain
https://arxiv.org/pdf/1807.01251.pdf El cual cita dos papers:
UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION
https://arxiv.org/pdf/1611.03530.pdf y Explicitizing an Implicit Bias of
the Frequency Principle in Two-layer Neural Networks
https://arxiv.org/pdf/1905.10264.pdf

\hypertarget{regularizaciuxf3n-l1-l2}{%
\subsection{Regularización L1, L2}\label{regularizaciuxf3n-l1-l2}}

\hypertarget{dropout}{%
\subsection{Dropout}\label{dropout}}

\hypertarget{normal-weight-initialization-as-bayesian-prior}{%
\subsection{Normal weight initialization as bayesian prior
(?)}\label{normal-weight-initialization-as-bayesian-prior}}

\hypertarget{otros-muxe9todos-de-regularizaciuxf3n}{%
\subsection{Otros métodos de
regularización}\label{otros-muxe9todos-de-regularizaciuxf3n}}

\hypertarget{early-stopping}{%
\subsubsection{Early stopping}\label{early-stopping}}

\hypertarget{data-augmentation}{%
\subsubsection{Data augmentation}\label{data-augmentation}}
