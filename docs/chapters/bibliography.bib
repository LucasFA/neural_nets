[Radford Neal Lecture notes in statistics
Bayesian learning for neural networks](https://link.springer.com/content/pdf/10.1007/978-1-4612-0745-0.pdf)

https://arxiv.org/pdf/2007.06823.pdf

https://arxiv.org/abs/1506.02142

@book{10.5555/1162264,
  author    = {Bishop, Christopher M.},
  title     = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  year      = {2006},
  isbn      = {0387310738},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg}
}


% https://www.deeplearningbook.org/
@book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  note      = {\url{http://www.deeplearningbook.org}},
  year      = {2016}
}
% https://ieeexplore.ieee.org/book/6267404 creo que no
@book{alma991014294651004990,
  publisher = {MIT Press},
  series    = {Neural network modeling and connectionism},
  title     = {Neural network design and the complexity of learning / J. Stephen Judd.},
  year      = {1990},
  abstract  = {Using the tools of complexity theory, Stephen Judd develops a formal description of associative learning in connectionist networks. He rigorously exposes the computational difficulties in training neural networks and explores how certain design principles will or will not make the problems easier.Judd looks beyond the scope of any one particular learning rule, at a level above the details of neurons. There he finds new issues that arise when great numbers of neurons are employed and he offers fresh insights into design principles that could guide the construction of artificial and biological neural networks.The first part of the book describes the motivations and goals of the study and relates them to current scientific theory. It provides an overview of the major ideas, formulates the general learning problem with an eye to the computational complexity of the task, reviews current theory on learning, relates the book's model of learning to other models outside the connectionist paradigm, and sets out to examine scale-up issues in connectionist learning.Later chapters prove the intractability of the general case of memorizing in networks, elaborate on implications of this intractability and point out several corollaries applying to various special subcases. Judd refines the distinctive characteristics of the difficulties with families of shallow networks, addresses concerns about the ability of neural networks to generalize, and summarizes the results, implications, and possible extensions of the work.J. Stephen Judd is Visiting Assistant Professor of Computer Science at The California Institute of Technology. Neural Network Design and the Complexity of Learning is included in the Network Modeling and Connectionism series edited by Jeffrey Elman.},
  author    = {Judd, J. Stephen},
  address   = {Cambridge, Massachusetts},
  booktitle = {Neural network design and the complexity of learning},
  isbn      = {9780262276559},
  keywords  = {Artificial intelligence},
  language  = {eng}
}

@misc{alma991014582939604990,
  publisher = {Pergamon},
  title     = {Neural networks [e-journal].},
  year      = {1988},
  address   = {Oxford ;},
  issn      = {1879-2782},
  journal   = {Neural networks (Online)},
  keywords  = {Neural computers$$QNeural computers -- Periodicals},
  language  = {eng},
  lccn      = {2005233410}
}
