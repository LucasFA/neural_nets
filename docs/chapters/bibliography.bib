# Página web (Wikipedia)
@misc{EulerWiki,
  author = {Wikipedia},
  title = {{L}eonhard {E}uler --- {W}ikipedia{,} The Free Encyclopedia},
  year = {2023},
  howpublished={\url{https://en.wikipedia.org/wiki/Leonhard_Euler}},
  url={\url{https://en.wikipedia.org/wiki/Leonhard_Euler}},
  note={[Recurso online, accedido el 27 de julio de 2023]}
}

# Libro
@book {Aigner2018,
    AUTHOR = {Aigner, Martin and Ziegler, G\"{u}nter M.},
     TITLE = {Proofs from {T}he {B}ook},
   EDITION = {Fifth},
      NOTE = {Including illustrations by Karl H. Hofmann},
 PUBLISHER = {Springer-Verlag, Berlin},
      YEAR = {2014},
     PAGES = {viii+308},
      ISBN = {978-3-662-44204-3; 978-3-662-44205-0},
       DOI = {10.1007/978-3-662-44205-0},
       URL = {https://doi.org/10.1007/978-3-662-44205-0},
}

# Artículo
@article {Euler1985,
    AUTHOR = {Euler, Leonhard},
     TITLE = {An essay on continued fractions},
      NOTE = {Translated from the Latin by B. F. Wyman and M. F. Wyman},
   JOURNAL = {Math. Systems Theory},
  FJOURNAL = {Mathematical Systems Theory. An International Journal on
              Mathematical Computing Theory},
    VOLUME = {18},
      YEAR = {1985},
    NUMBER = {4},
     PAGES = {295--328},
      ISSN = {0025-5661},
       DOI = {10.1007/BF01699475},
       URL = {https://doi.org/10.1007/BF01699475},
}

# Tesis
@phdthesis{CitekeyPhdthesis,
  author  = {Rempel, Robert Charles},
  title   = {Relaxation Effects for Coupled Nuclear Spins},
  school  = {Stanford University},
  address = {Stanford, CA},
  year    = 1956,
  month   = jun
}

# Trabajo fin de máster
@mastersthesis{CitekeyMastersthesis,
  author  = {Jian Tang},
  title   = {Spin structure of the nucleon in the asymptotic limit},
  school  = {Massachusetts Institute of Technology},
  year    = 1996,
  address = {Cambridge, MA},
  month   = sep
}

# Trabajo fin de grado
@thesis{CiteKeyBachelorsthesis,
  author  = {Doe, John},
  title   = {Are we living in a simulation?},
  school  = {Massachusetts Institute of Technology},
  year    = 2003,
  address = {Cambridge, MA},
  month   = jul,
  type    = {Bachelor's Thesis},
  note    = {Bacherlo's Thesis, Massachusetts Institute of Technology, Cambridge, MA.}
}

# Artículo no publicado
@misc{castroinfantes2022conjugate,
  title         =  {Conjugate Plateau constructions in product spaces}, 
  author        =  {Jesús Castro-Infantes and José M. Manzano and Francisco Torralbo},
  year          =  {2022},
  eprint        =  {2203.13162},
  archivePrefix =  {arXiv},
  primaryClass  =  {math.DG},
  note          =  {Preprint. arXiv: 2203.13162 [math.DG]}
}

[Radford Neal Lecture notes in statistics
Bayesian learning for neural networks](https://link.springer.com/content/pdf/10.1007/978-1-4612-0745-0.pdf)

https://arxiv.org/pdf/2007.06823.pdf

https://arxiv.org/abs/1506.02142

% recurso principal ahora mismo
% https://www.deeplearningbook.org/
@book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  note      = {\url{http://www.deeplearningbook.org}},
  year      = {2016}
}


% https://ieeexplore.ieee.org/book/6267404 creo que no
@book{alma991014294651004990,
  publisher = {MIT Press},
  series    = {Neural network modeling and connectionism},
  title     = {Neural network design and the complexity of learning / J. Stephen Judd.},
  year      = {1990},
  abstract  = {Using the tools of complexity theory, Stephen Judd develops a formal description of associative learning in connectionist networks. He rigorously exposes the computational difficulties in training neural networks and explores how certain design principles will or will not make the problems easier.Judd looks beyond the scope of any one particular learning rule, at a level above the details of neurons. There he finds new issues that arise when great numbers of neurons are employed and he offers fresh insights into design principles that could guide the construction of artificial and biological neural networks.The first part of the book describes the motivations and goals of the study and relates them to current scientific theory. It provides an overview of the major ideas, formulates the general learning problem with an eye to the computational complexity of the task, reviews current theory on learning, relates the book's model of learning to other models outside the connectionist paradigm, and sets out to examine scale-up issues in connectionist learning.Later chapters prove the intractability of the general case of memorizing in networks, elaborate on implications of this intractability and point out several corollaries applying to various special subcases. Judd refines the distinctive characteristics of the difficulties with families of shallow networks, addresses concerns about the ability of neural networks to generalize, and summarizes the results, implications, and possible extensions of the work.J. Stephen Judd is Visiting Assistant Professor of Computer Science at The California Institute of Technology. Neural Network Design and the Complexity of Learning is included in the Network Modeling and Connectionism series edited by Jeffrey Elman.},
  author    = {Judd, J. Stephen},
  address   = {Cambridge, Massachusetts},
  booktitle = {Neural network design and the complexity of learning},
  isbn      = {9780262276559},
  keywords  = {Artificial intelligence},
  language  = {eng}
}

@misc{alma991014582939604990,
  publisher = {Pergamon},
  title     = {Neural networks [e-journal].},
  year      = {1988},
  address   = {Oxford ;},
  issn      = {1879-2782},
  journal   = {Neural networks (Online)},
  keywords  = {Neural computers$$QNeural computers -- Periodicals},
  language  = {eng},
  lccn      = {2005233410}
}
