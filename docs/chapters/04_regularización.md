# Regularización

## Motivación

<!-- TODO comprobar ecuación -->
Overfitting is blah blah, ecuación de inferencia MSE = bias ^ 2 + varianza  paper de Yann Le Cun de Learning in high dimensions always amounts to extrapolation (according to the paper even if it's a low-dimensional data manifold! TBResearched)

Wikipedia da enlaces a ciertos papers interesantes sobre el "sesgo espectral" - aparentemente las redes neuronales tienden a aproximar primero frecuencias bajas - en el sentido de Fourier:
On the Spectral Bias of Neural Networks
https://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf

## Regularización L1, L2

## Dropout

## Normal weight initialization as bayesian prior (?)

## Otros métodos de regularización

### Early stopping

### Data augmentation
